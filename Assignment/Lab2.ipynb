{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import logging.handlers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch.nn.parallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "wandb.init(\n",
    "    project=\"DL_Lab2\",\n",
    "    config={\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-10\",\n",
    "    \"epochs\": 20,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)   \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 500)       \n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))          \n",
    "        x = x.view(-1, 256 * 2 * 2)                  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, test_loader, criterion, use_cuda):\n",
    "        # calculate the accuracy on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for data, target in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            for i in range(len(target.data)):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        for i in range(10):\n",
    "            if class_total[i] > 0:\n",
    "                # log accuracy of each class\n",
    "                wandb.log({\"acc_{}\".format(classes[i]): class_correct[i] / class_total[i]})\n",
    "                #print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            else:\n",
    "                print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))  \n",
    "        #wandb log the average acc\n",
    "        wandb.log({\"acc\": np.sum(class_correct) / np.sum(class_total)})\n",
    "        # print average acc\n",
    "        print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "        \n",
    "\n",
    "    def train_model(model, train_loader, valid_loader, epochs, optimizer, criterion, use_cuda, save_path):\n",
    "        valid_loss_min = np.Inf\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()\n",
    "            for data, target in train_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "            model.eval()\n",
    "            for data, target in valid_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss/len(train_loader.sampler)\n",
    "            valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "            model.evaluate( valid_loader, criterion, use_cuda)\n",
    "            wandb.log({\"training_loss\": train_loss, \"val_loss\": valid_loss})\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                valid_loss_min = valid_loss\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(model, test_loader, use_cuda):\n",
    "        model.eval()\n",
    "        test_preds = torch.LongTensor()\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        for i, data in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            output = model(data)\n",
    "            preds = output.cpu().data.max(1, keepdim=True)[1]\n",
    "            test_preds = torch.cat((test_preds, preds), dim=0)\n",
    "        return test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 标准化\n",
    "])\n",
    "batch_size = 128\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the train and test dataset sizes\n",
    "print('Train: ', len(train_loader.dataset))\n",
    "print('Test: ', len(test_loader.dataset))\n",
    "# print the train and test batch sizes\n",
    "print('Train Batch Size: ', len(train_loader))\n",
    "print('Test Batch Size: ', len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each class number of train dataset\n",
    "for i in range(10):\n",
    "    print('Number of %5s: %5d' % (classes[i], (np.array(train_dataset.targets) == i).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label the classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# show the second image and the label\n",
    "image, label = train_dataset[1]\n",
    "image = image.permute(1, 2, 0)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(classes[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Net to train the model on CIFAR-10 define a instance of Net\n",
    "'''\n",
    "model = Net()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,  weight_decay=1e-4)\n",
    "epochs = 50\n",
    "Net.train_model(model, train_loader, test_loader, epochs, optimizer, criterion, use_cuda, 'model.pt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "# model.evaluate(test_loader, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a CNN with resiudal block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, dropout_rate=0.7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self.make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out) \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "    def evaluate(self, test_loader, criterion, use_cuda):\n",
    "        # calculate the accuracy on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for data, target in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            for i in range(len(target.data)):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        for i in range(10):\n",
    "            if class_total[i] > 0:\n",
    "                # log accuracy of each class\n",
    "                wandb.log({\"acc_{}\".format(classes[i]): class_correct[i] / class_total[i]})\n",
    "                #print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            else:\n",
    "                print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))  \n",
    "        #wandb log the average acc\n",
    "        wandb.log({\"acc\": np.sum(class_correct) / np.sum(class_total)})\n",
    "        # print average acc\n",
    "        print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "        \n",
    "\n",
    "    def train_model(model, train_loader, valid_loader, epochs, optimizer, criterion, use_cuda, save_path):\n",
    "        valid_loss_min = np.Inf\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()\n",
    "            for data, target in train_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "            model.eval()\n",
    "            for data, target in valid_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss/len(train_loader.sampler)\n",
    "            valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "            model.evaluate( valid_loader, criterion, use_cuda)\n",
    "            wandb.log({\"training_loss\": train_loss, \"val_loss\": valid_loss})\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                valid_loss_min = valid_loss\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(model, test_loader, use_cuda):\n",
    "        model.eval()\n",
    "        test_preds = torch.LongTensor()\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        for i, data in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            output = model(data)\n",
    "            preds = output.cpu().data.max(1, keepdim=True)[1]\n",
    "            test_preds = torch.cat((test_preds, preds), dim=0)\n",
    "        return test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ResNet architecture\n",
    "'''\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "model = ResNet(ResidualBlock, [3, 4, 6, 4])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "epochs = 50\n",
    "ResNet.train_model(model, train_loader, test_loader, epochs, optimizer, criterion, use_cuda, 'ResNet_model.pt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model\n",
    "# model.evaluate(test_loader, criterion, use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印网络结构\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement of DenseNet\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, dropout_rate=0.75):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.cat([out, x], 1)\n",
    "        return out  \n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_blocks, growth_rate=64, reduction=0.8, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        num_channels = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_channels, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        self.dense1 = self.make_dense_layers(Bottleneck, num_channels, growth_rate, num_blocks[0])\n",
    "        num_channels += num_blocks[0] * growth_rate\n",
    "        out_channels = int(reduction * num_channels)\n",
    "        self.trans1 = Transition(num_channels, out_channels)\n",
    "\n",
    "        num_channels = out_channels\n",
    "        self.dense2 = self.make_dense_layers(Bottleneck, num_channels, growth_rate, num_blocks[1])\n",
    "        num_channels += num_blocks[1] * growth_rate\n",
    "        out_channels = int(reduction * num_channels)\n",
    "        self.trans2 = Transition(num_channels, out_channels)\n",
    "\n",
    "        num_channels = out_channels\n",
    "        self.dense3 = self.make_dense_layers(Bottleneck, num_channels, growth_rate, num_blocks[2])\n",
    "        num_channels += num_blocks[2] * growth_rate\n",
    "        out_channels = int(reduction * num_channels)\n",
    "        self.trans3 = Transition(num_channels, out_channels)\n",
    "\n",
    "        num_channels = out_channels\n",
    "        self.dense4 = self.make_dense_layers(Bottleneck, num_channels, growth_rate, num_blocks[3])\n",
    "        num_channels += num_blocks[3] * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_channels)\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def make_dense_layers(self, block, in_channels, growth_rate, num_blocks):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(block(in_channels + i * growth_rate, growth_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.dense1(out)\n",
    "        out = self.trans1(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.trans2(out)\n",
    "        out = self.dense3(out)\n",
    "        out = self.trans3(out)\n",
    "        out = self.dense4(out)\n",
    "        out = F.relu(self.bn(out))\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def evaluate(self, test_loader, criterion, use_cuda):\n",
    "        # calculate the accuracy on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for data, target in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            for i in range(len(target.data)):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        for i in range(10):\n",
    "            if class_total[i] > 0:\n",
    "                # log accuracy of each class\n",
    "                wandb.log({\"acc_{}\".format(classes[i]): class_correct[i] / class_total[i]})\n",
    "                #print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            else:\n",
    "                print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))  \n",
    "        #wandb log the average acc\n",
    "        wandb.log({\"acc\": np.sum(class_correct) / np.sum(class_total)})\n",
    "        # print average acc\n",
    "        print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "        \n",
    "\n",
    "    def train_model(model, train_loader, valid_loader, epochs, optimizer, criterion, use_cuda, save_path):\n",
    "        valid_loss_min = np.Inf\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()\n",
    "            for data, target in train_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "            model.eval()\n",
    "            for data, target in valid_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss/len(train_loader.sampler)\n",
    "            valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "            model.evaluate( valid_loader, criterion, use_cuda)\n",
    "            wandb.log({\"training_loss\": train_loss, \"val_loss\": valid_loss})\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                valid_loss_min = valid_loss\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(model, test_loader, use_cuda):\n",
    "        model.eval()\n",
    "        test_preds = torch.LongTensor()\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        for i, data in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            output = model(data)\n",
    "            preds = output.cpu().data.max(1, keepdim=True)[1]\n",
    "            test_preds = torch.cat((test_preds, preds), dim=0)\n",
    "        return test_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the DenseNet architecture\n",
    "'''\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "model = DenseNet([6, 12, 24, 48])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=5e-3)\n",
    "epochs = 50\n",
    "DenseNet.train_model(model, train_loader, test_loader, epochs, optimizer, criterion, use_cuda, 'DenseNet_model.pt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "# model.evaluate(test_loader, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a CNN with resiudal block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualBlockWithSE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlockWithSE, self).__init__()\n",
    "        # Residual block components\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.se = SEBlock(out_channels)  # SE Block\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)  # Apply SE block\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ResNet_SE(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, dropout_rate=0.7):\n",
    "        super(ResNet_SE, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self.make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out) \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "    def evaluate(self, test_loader, criterion, use_cuda):\n",
    "        # calculate the accuracy on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for data, target in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            for i in range(len(target.data)):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        for i in range(10):\n",
    "            if class_total[i] > 0:\n",
    "                # log accuracy of each class\n",
    "                wandb.log({\"acc_{}\".format(classes[i]): class_correct[i] / class_total[i]})\n",
    "                #print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            else:\n",
    "                print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))  \n",
    "        #wandb log the average acc\n",
    "        wandb.log({\"acc\": np.sum(class_correct) / np.sum(class_total)})\n",
    "        # print average acc\n",
    "        print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "        \n",
    "\n",
    "    def train_model(model, train_loader, valid_loader, epochs, optimizer, criterion, use_cuda, save_path):\n",
    "        valid_loss_min = np.Inf\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()\n",
    "            for data, target in train_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "            model.eval()\n",
    "            for data, target in valid_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss/len(train_loader.sampler)\n",
    "            valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "            model.evaluate( valid_loader, criterion, use_cuda)\n",
    "            wandb.log({\"training_loss\": train_loss, \"val_loss\": valid_loss})\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                valid_loss_min = valid_loss\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(model, test_loader, use_cuda):\n",
    "        model.eval()\n",
    "        test_preds = torch.LongTensor()\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        for i, data in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            output = model(data)\n",
    "            preds = output.cpu().data.max(1, keepdim=True)[1]\n",
    "            test_preds = torch.cat((test_preds, preds), dim=0)\n",
    "        return test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ResNet_SE architecture\n",
    "model = ResNet_SE(ResidualBlockWithSE, [6, 8, 12, 8])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=5e-4)\n",
    "epochs = 50\n",
    "ResNet_SE.train_model(model, train_loader, test_loader, epochs, optimizer, criterion, use_cuda, 'ResNet_SE_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model.evaluate(test_loader, criterion, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
