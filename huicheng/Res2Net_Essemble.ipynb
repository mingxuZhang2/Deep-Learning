{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingxu/miniconda3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load CIFAR-100 dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import transforms\n",
    "# import neccessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import logging.handlers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch.nn.parallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
      "Class 0 has apple, count: 1000 samples\n",
      "Class 1 has aquarium_fish, count: 1000 samples\n",
      "Class 2 has baby, count: 1000 samples\n",
      "Class 3 has bear, count: 1000 samples\n",
      "Class 4 has beaver, count: 1000 samples\n",
      "Class 5 has bed, count: 1000 samples\n",
      "Class 6 has bee, count: 1000 samples\n",
      "Class 7 has beetle, count: 1000 samples\n",
      "Class 8 has bicycle, count: 1000 samples\n",
      "Class 9 has bottle, count: 1000 samples\n",
      "Class 10 has bowl, count: 1000 samples\n",
      "Class 11 has boy, count: 1000 samples\n",
      "Class 12 has bridge, count: 1000 samples\n",
      "Class 13 has bus, count: 1000 samples\n",
      "Class 14 has butterfly, count: 1000 samples\n",
      "Class 15 has camel, count: 1000 samples\n",
      "Class 16 has can, count: 1000 samples\n",
      "Class 17 has castle, count: 1000 samples\n",
      "Class 18 has caterpillar, count: 1000 samples\n",
      "Class 19 has cattle, count: 1000 samples\n",
      "Class 20 has chair, count: 1000 samples\n",
      "Class 21 has chimpanzee, count: 1000 samples\n",
      "Class 22 has clock, count: 1000 samples\n",
      "Class 23 has cloud, count: 1000 samples\n",
      "Class 24 has cockroach, count: 1000 samples\n",
      "Class 25 has couch, count: 1000 samples\n",
      "Class 26 has crab, count: 1000 samples\n",
      "Class 27 has crocodile, count: 1000 samples\n",
      "Class 28 has cup, count: 1000 samples\n",
      "Class 29 has dinosaur, count: 1000 samples\n",
      "Class 30 has dolphin, count: 1000 samples\n",
      "Class 31 has elephant, count: 1000 samples\n",
      "Class 32 has flatfish, count: 1000 samples\n",
      "Class 33 has forest, count: 1000 samples\n",
      "Class 34 has fox, count: 1000 samples\n",
      "Class 35 has girl, count: 1000 samples\n",
      "Class 36 has hamster, count: 1000 samples\n",
      "Class 37 has house, count: 1000 samples\n",
      "Class 38 has kangaroo, count: 1000 samples\n",
      "Class 39 has keyboard, count: 1000 samples\n",
      "Class 40 has lamp, count: 1000 samples\n",
      "Class 41 has lawn_mower, count: 1000 samples\n",
      "Class 42 has leopard, count: 1000 samples\n",
      "Class 43 has lion, count: 1000 samples\n",
      "Class 44 has lizard, count: 1000 samples\n",
      "Class 45 has lobster, count: 1000 samples\n",
      "Class 46 has man, count: 1000 samples\n",
      "Class 47 has maple_tree, count: 1000 samples\n",
      "Class 48 has motorcycle, count: 1000 samples\n",
      "Class 49 has mountain, count: 1000 samples\n",
      "Class 50 has mouse, count: 1000 samples\n",
      "Class 51 has mushroom, count: 1000 samples\n",
      "Class 52 has oak_tree, count: 1000 samples\n",
      "Class 53 has orange, count: 1000 samples\n",
      "Class 54 has orchid, count: 1000 samples\n",
      "Class 55 has otter, count: 1000 samples\n",
      "Class 56 has palm_tree, count: 1000 samples\n",
      "Class 57 has pear, count: 1000 samples\n",
      "Class 58 has pickup_truck, count: 1000 samples\n",
      "Class 59 has pine_tree, count: 1000 samples\n",
      "Class 60 has plain, count: 1000 samples\n",
      "Class 61 has plate, count: 1000 samples\n",
      "Class 62 has poppy, count: 1000 samples\n",
      "Class 63 has porcupine, count: 1000 samples\n",
      "Class 64 has possum, count: 1000 samples\n",
      "Class 65 has rabbit, count: 1000 samples\n",
      "Class 66 has raccoon, count: 1000 samples\n",
      "Class 67 has ray, count: 1000 samples\n",
      "Class 68 has road, count: 1000 samples\n",
      "Class 69 has rocket, count: 1000 samples\n",
      "Class 70 has rose, count: 1000 samples\n",
      "Class 71 has sea, count: 1000 samples\n",
      "Class 72 has seal, count: 1000 samples\n",
      "Class 73 has shark, count: 1000 samples\n",
      "Class 74 has shrew, count: 1000 samples\n",
      "Class 75 has skunk, count: 1000 samples\n",
      "Class 76 has skyscraper, count: 1000 samples\n",
      "Class 77 has snail, count: 1000 samples\n",
      "Class 78 has snake, count: 1000 samples\n",
      "Class 79 has spider, count: 1000 samples\n",
      "Class 80 has squirrel, count: 1000 samples\n",
      "Class 81 has streetcar, count: 1000 samples\n",
      "Class 82 has sunflower, count: 1000 samples\n",
      "Class 83 has sweet_pepper, count: 1000 samples\n",
      "Class 84 has table, count: 1000 samples\n",
      "Class 85 has tank, count: 1000 samples\n",
      "Class 86 has telephone, count: 1000 samples\n",
      "Class 87 has television, count: 1000 samples\n",
      "Class 88 has tiger, count: 1000 samples\n",
      "Class 89 has tractor, count: 1000 samples\n",
      "Class 90 has train, count: 1000 samples\n",
      "Class 91 has trout, count: 1000 samples\n",
      "Class 92 has tulip, count: 1000 samples\n",
      "Class 93 has turtle, count: 1000 samples\n",
      "Class 94 has wardrobe, count: 1000 samples\n",
      "Class 95 has whale, count: 1000 samples\n",
      "Class 96 has willow_tree, count: 1000 samples\n",
      "Class 97 has wolf, count: 1000 samples\n",
      "Class 98 has woman, count: 1000 samples\n",
      "Class 99 has worm, count: 1000 samples\n"
     ]
    }
   ],
   "source": [
    "transform_ori = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 标准化\n",
    "])\n",
    "\n",
    "class CustomCIFAR100(Dataset):\n",
    "    def __init__(self, root, train, download, transform=None):\n",
    "        # 原始数据集\n",
    "        self.original_dataset = datasets.CIFAR100(root=root, train=train, download=download, transform=transform_ori)\n",
    "        # 只在增强数据集上应用变换\n",
    "        self.augmented_dataset = datasets.CIFAR100(root=root, train=train, download=False, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据集大小翻倍\n",
    "        return 2 * len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx % 2 == 0:  # 偶数索引返回增强数据\n",
    "            return self.augmented_dataset[idx // 2]\n",
    "        else:  # 奇数索引返回原始数据\n",
    "            return self.original_dataset[idx // 2]\n",
    "\n",
    "# 定义增强变换\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Compose([transforms.RandomPerspective(distortion_scale=0.5)]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = CustomCIFAR100(root='./data', train=True, download=True, transform=augmentations)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_ori)\n",
    "\n",
    "# 首先获取类别信息\n",
    "classes = train_dataset.original_dataset.classes\n",
    "print(classes)\n",
    "\n",
    "# 初始化类计数器\n",
    "class_counts = {i: 0 for i in range(100)}\n",
    "\n",
    "# 遍历原始数据集的目标标签，每找到一个标签，计数加2（原始和增强）\n",
    "for label in train_dataset.original_dataset.targets:\n",
    "    class_counts[label] += 2  # 因为我们添加了原始和对应的增强图像\n",
    "\n",
    "# 打印每个类别的样本数\n",
    "for i in range(100):\n",
    "    print(f\"Class {i} has {classes[i]}, count: {class_counts[i]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128 load dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  100000\n",
      "Test:  10000\n",
      "Total Batch:  782\n",
      "Total Batch:  79\n"
     ]
    }
   ],
   "source": [
    "# print the train and test dataset sizes\n",
    "print('Train: ', len(train_loader.dataset))\n",
    "print('Test: ', len(test_loader.dataset))\n",
    "# print the train and test batch sizes\n",
    "print('Total Batch: ', len(train_loader))\n",
    "print('Total Batch: ', len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmingxu_zhang\u001b[0m (\u001b[33mmingxus-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/mingxu/DL/Final_work/Deep-Learning/mingxu/wandb/run-20240605_002355-ljjkpnx4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100/runs/ljjkpnx4' target=\"_blank\">peach-jazz-81</a></strong> to <a href='https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100' target=\"_blank\">https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100/runs/ljjkpnx4' target=\"_blank\">https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100/runs/ljjkpnx4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mingxus-team/DL_Classification_CIFAR-100/runs/ljjkpnx4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7feac35e5810>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "wandb.init(\n",
    "    project=\"DL_Classification_CIFAR-100\",\n",
    "    config={\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"architecture\": \"Res2Net_enhance\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 50,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2NetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, scale=4):\n",
    "        super(Res2NetBlock, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.width = out_channels // scale\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(self.width, self.width, kernel_size=3, stride=1, padding=1, bias=False) for _ in range(scale - 1)\n",
    "        ])\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm2d(self.width) for _ in range(scale - 1)\n",
    "        ])\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "\n",
    "\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        spx = list(spx)\n",
    "        out = self.convs[0](spx[0])\n",
    "        out = self.relu(self.bns[0](out))\n",
    "        \n",
    "\n",
    "        for i in range(1, self.scale - 1):\n",
    "            sp = self.convs[i](spx[i])\n",
    "            sp = self.relu(self.bns[i](sp))\n",
    "            out = torch.cat((out, sp), 1)\n",
    " \n",
    "\n",
    "        if self.scale != 1:\n",
    "            out = torch.cat((out, spx[-1]), 1)  # Adding the residual part of split not processed\n",
    "\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.shortcut is not None:\n",
    "            residual = self.shortcut(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100, dropout_rate=0.5, scale=4):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self.make_layer(block, 32, num_blocks[0], stride=1, scale=scale)\n",
    "        self.layer2 = self.make_layer(block, 64, num_blocks[1], stride=2, scale=scale)\n",
    "        self.layer3 = self.make_layer(block, 128, num_blocks[2], stride=2, scale=scale)\n",
    "        self.layer4 = self.make_layer(block, 256, num_blocks[3], stride=2, scale=scale)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, num_blocks, stride, scale):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride, scale))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "    def evaluate(self, test_loader, criterion, use_cuda, index):\n",
    "        # calculate the accuracy on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        class_correct = list(0. for i in range(100))\n",
    "        class_total = list(0. for i in range(100))\n",
    "        for data, target in tqdm(test_loader):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            for i in range(len(target.data)):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        for i in range(100):\n",
    "            if class_total[i] > 0:\n",
    "                # log accuracy of each class\n",
    "                # wandb.log({\"acc_{}\".format(classes[i]): class_correct[i] / class_total[i]})\n",
    "                wandb.log({\"acc_model_{}\".format(index): class_correct[i] / class_total[i]})\n",
    "                #print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            else:\n",
    "                print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))  \n",
    "        #wandb log the average acc\n",
    "        # wandb.log({\"acc\": np.sum(class_correct) / np.sum(class_total)})\n",
    "        # print average acc\n",
    "        print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))\n",
    "        \n",
    "\n",
    "    def train_model(model, train_loader, valid_loader, epochs, optimizer, criterion, use_cuda, save_path):\n",
    "        valid_loss_min = np.Inf\n",
    "        count = 0\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()\n",
    "            for data, target in tqdm(train_loader):\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "            model.eval()\n",
    "            for data, target in valid_loader:\n",
    "                if use_cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss/len(train_loader.sampler)\n",
    "            valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "            model.evaluate( valid_loader, criterion, use_cuda)\n",
    "            wandb.log({\"training_loss\": train_loss, \"val_loss\": valid_loss})  \n",
    "\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                valid_loss_min = valid_loss\n",
    "                count = 0\n",
    "            else :\n",
    "                count = count + 1\n",
    "                if count > 10:\n",
    "                    break\n",
    "            torch.save(model.state_dict(), save_path + str(epoch%5))\n",
    "\n",
    "    def predict(model, test_loader, use_cuda):\n",
    "        model.eval()  # Ensure the model is in evaluation mode\n",
    "        predictions = []\n",
    "        with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "            for data, _ in tqdm(test_loader):\n",
    "                if use_cuda:\n",
    "                    data = data.cuda()  # Move data to GPU if CUDA is enabled\n",
    "                output = model(data)\n",
    "                _, pred = torch.max(output, 1)  # Get the index of the max log-probability\n",
    "                pred = pred.cpu().numpy() if use_cuda else pred.numpy()  # Move data to CPU if CUDA is used\n",
    "                predictions.extend(pred)\n",
    "        return predictions\n",
    "\n",
    "    def predict_poss(model, test_loader, use_cuda):\n",
    "        model.eval()  # Ensure the model is in evaluation mode\n",
    "        probabilities = []\n",
    "        with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "            for data, _ in tqdm(test_loader):\n",
    "                if use_cuda:\n",
    "                    data = data.cuda()  # Move data to GPU if CUDA is enabled\n",
    "\n",
    "                output = model(data)\n",
    "                prob = F.softmax(output, dim=1)  # Compute the probability distribution over classes\n",
    "\n",
    "                if use_cuda:\n",
    "                    prob = prob.cpu()  # Move data to CPU if CUDA is used\n",
    "\n",
    "                probabilities.append(prob.numpy())  # Store the probabilities\n",
    "\n",
    "        # 使用 np.vstack 以确保即使批量大小不同也能正确合并\n",
    "        return np.vstack(probabilities)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Res2NetBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): Res2NetBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Res2NetBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Res2NetBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Res2NetBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Res2NetBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Res2NetBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Res2NetBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Res2NetBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Res2NetBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Res2NetBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Res2NetBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (6): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Res2NetBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (6): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (7): Res2NetBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (convs): ModuleList(\n",
      "        (0-2): 3 x Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0-2): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_blocks = [5, 6, 7, 8]\n",
    "model = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model0 = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "model1 = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "model2 = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "model3 = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "model4 = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model0 = model0.cuda()\n",
    "    model1 = model1.cuda()\n",
    "    model2 = model2.cuda()\n",
    "    model3 = model3.cuda()\n",
    "    model4 = model4.cuda()\n",
    "\n",
    "# load model\n",
    "model0.load_state_dict(torch.load('Res2Net_baseline_model_enhance0.pt'))\n",
    "model1.load_state_dict(torch.load('Res2Net_baseline_model_enhance1.pt'))\n",
    "model2.load_state_dict(torch.load('Res2Net_baseline_model_enhance2.pt'))\n",
    "model3.load_state_dict(torch.load('Res2Net_baseline_model_enhance3.pt'))\n",
    "model4.load_state_dict(torch.load('Res2Net_baseline_model_enhance4.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\\nuse_cuda = torch.cuda.is_available()\\nif use_cuda:\\n    model = model.cuda()\\n# train model\\ncriterion = nn.CrossEntropyLoss()\\nmodel.load_state_dict(torch.load('Res2Net_baseline_model_enhance4.pt'))\\nmodel.evaluate(test_loader, criterion, use_cuda, 'Res2Net_baseline_enhance4')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = ResNet(Res2NetBlock, num_blocks, num_classes=100, dropout_rate=0, scale=4)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "# train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load('Res2Net_baseline_model_enhance4.pt'))\n",
    "model.evaluate(test_loader, criterion, use_cuda, 'Res2Net_baseline_enhance4')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel0.eval()\\nmodel1.eval()\\nmodel2.eval()\\nmodel3.eval()\\nmodel4.eval()\\n\\ntest_preds0 = torch.tensor(model0.predict(test_loader, use_cuda), dtype=torch.long)\\ntest_preds1 = torch.tensor(model1.predict(test_loader, use_cuda), dtype=torch.long)\\ntest_preds2 = torch.tensor(model2.predict(test_loader, use_cuda), dtype=torch.long)\\ntest_preds3 = torch.tensor(model3.predict(test_loader, use_cuda), dtype=torch.long)\\ntest_preds4 = torch.tensor(model4.predict(test_loader, use_cuda), dtype=torch.long)\\n\\nstacked_preds = torch.stack([test_preds0, test_preds1, test_preds2, test_preds3, test_preds4], dim=0)\\n\\n# Apply torch.mode to find the most common prediction along the stacked dimension\\ntest_preds = torch.mode(stacked_preds, dim=0).values\\n\\n# Ensure that the dataset has an attribute \\'targets\\' or adjust according to your dataset structure\\nclass_correct = list(0. for i in range(100))\\nclass_total = list(0. for i in range(100))\\nfor i in range(len(test_loader.dataset.targets)):\\n    label = test_loader.dataset.targets[i]\\n    class_correct[label] += int(test_preds[i].item() == label)\\n\\n    class_total[label] += 1\\n\\n# Log ensemble accuracies to wandb and print them\\nfor i in range(100):\\n    if class_total[i] > 0:\\n        wandb.log({\"res2net_acc_ensemble_voting5\": class_correct[i] / class_total[i]})\\n        print(\\'Test Accuracy of %5s: %2d%% (%2d/%2d)\\' % (str(i), 100 * class_correct[i] / class_total[i], class_correct[i], class_total[i]))\\n    else:\\n        print(\\'Test Accuracy of %5s: N/A (no training examples)\\' % i)\\n\\n# Calculate and print overall accuracy\\nprint(\\'Test Accuracy (Overall): %2d%% (%2d/%2d)\\' % (100. * sum(class_correct) / sum(class_total), sum(class_correct), sum(class_total)))\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用多数投票的方式进行集成\n",
    "'''\n",
    "model0.eval()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "\n",
    "test_preds0 = torch.tensor(model0.predict(test_loader, use_cuda), dtype=torch.long)\n",
    "test_preds1 = torch.tensor(model1.predict(test_loader, use_cuda), dtype=torch.long)\n",
    "test_preds2 = torch.tensor(model2.predict(test_loader, use_cuda), dtype=torch.long)\n",
    "test_preds3 = torch.tensor(model3.predict(test_loader, use_cuda), dtype=torch.long)\n",
    "test_preds4 = torch.tensor(model4.predict(test_loader, use_cuda), dtype=torch.long)\n",
    "\n",
    "stacked_preds = torch.stack([test_preds0, test_preds1, test_preds2, test_preds3, test_preds4], dim=0)\n",
    "\n",
    "# Apply torch.mode to find the most common prediction along the stacked dimension\n",
    "test_preds = torch.mode(stacked_preds, dim=0).values\n",
    "\n",
    "# Ensure that the dataset has an attribute 'targets' or adjust according to your dataset structure\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "for i in range(len(test_loader.dataset.targets)):\n",
    "    label = test_loader.dataset.targets[i]\n",
    "    class_correct[label] += int(test_preds[i].item() == label)\n",
    "\n",
    "    class_total[label] += 1\n",
    "\n",
    "# Log ensemble accuracies to wandb and print them\n",
    "for i in range(100):\n",
    "    if class_total[i] > 0:\n",
    "        wandb.log({\"res2net_acc_ensemble_voting5\": class_correct[i] / class_total[i]})\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], class_correct[i], class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % i)\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * sum(class_correct) / sum(class_total), sum(class_correct), sum(class_total)))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 17.69it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 26.03it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 26.08it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 25.83it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 25.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of     0: 85% (85/100)\n",
      "Test Accuracy of     1: 80% (80/100)\n",
      "Test Accuracy of     2: 56% (56/100)\n",
      "Test Accuracy of     3: 48% (48/100)\n",
      "Test Accuracy of     4: 54% (54/100)\n",
      "Test Accuracy of     5: 58% (58/100)\n",
      "Test Accuracy of     6: 77% (77/100)\n",
      "Test Accuracy of     7: 72% (72/100)\n",
      "Test Accuracy of     8: 85% (85/100)\n",
      "Test Accuracy of     9: 80% (80/100)\n",
      "Test Accuracy of    10: 52% (52/100)\n",
      "Test Accuracy of    11: 45% (45/100)\n",
      "Test Accuracy of    12: 75% (75/100)\n",
      "Test Accuracy of    13: 54% (54/100)\n",
      "Test Accuracy of    14: 53% (53/100)\n",
      "Test Accuracy of    15: 71% (71/100)\n",
      "Test Accuracy of    16: 69% (69/100)\n",
      "Test Accuracy of    17: 82% (82/100)\n",
      "Test Accuracy of    18: 63% (63/100)\n",
      "Test Accuracy of    19: 53% (53/100)\n",
      "Test Accuracy of    20: 80% (80/100)\n",
      "Test Accuracy of    21: 89% (89/100)\n",
      "Test Accuracy of    22: 58% (58/100)\n",
      "Test Accuracy of    23: 84% (84/100)\n",
      "Test Accuracy of    24: 80% (80/100)\n",
      "Test Accuracy of    25: 56% (56/100)\n",
      "Test Accuracy of    26: 57% (57/100)\n",
      "Test Accuracy of    27: 52% (52/100)\n",
      "Test Accuracy of    28: 74% (74/100)\n",
      "Test Accuracy of    29: 64% (64/100)\n",
      "Test Accuracy of    30: 57% (57/100)\n",
      "Test Accuracy of    31: 70% (70/100)\n",
      "Test Accuracy of    32: 64% (64/100)\n",
      "Test Accuracy of    33: 63% (63/100)\n",
      "Test Accuracy of    34: 68% (68/100)\n",
      "Test Accuracy of    35: 47% (47/100)\n",
      "Test Accuracy of    36: 64% (64/100)\n",
      "Test Accuracy of    37: 63% (63/100)\n",
      "Test Accuracy of    38: 55% (55/100)\n",
      "Test Accuracy of    39: 80% (80/100)\n",
      "Test Accuracy of    40: 57% (57/100)\n",
      "Test Accuracy of    41: 86% (86/100)\n",
      "Test Accuracy of    42: 68% (68/100)\n",
      "Test Accuracy of    43: 68% (68/100)\n",
      "Test Accuracy of    44: 47% (47/100)\n",
      "Test Accuracy of    45: 56% (56/100)\n",
      "Test Accuracy of    46: 35% (35/100)\n",
      "Test Accuracy of    47: 67% (67/100)\n",
      "Test Accuracy of    48: 83% (83/100)\n",
      "Test Accuracy of    49: 81% (81/100)\n",
      "Test Accuracy of    50: 50% (50/100)\n",
      "Test Accuracy of    51: 68% (68/100)\n",
      "Test Accuracy of    52: 46% (46/100)\n",
      "Test Accuracy of    53: 90% (90/100)\n",
      "Test Accuracy of    54: 81% (81/100)\n",
      "Test Accuracy of    55: 42% (42/100)\n",
      "Test Accuracy of    56: 82% (82/100)\n",
      "Test Accuracy of    57: 72% (72/100)\n",
      "Test Accuracy of    58: 74% (74/100)\n",
      "Test Accuracy of    59: 65% (65/100)\n",
      "Test Accuracy of    60: 85% (85/100)\n",
      "Test Accuracy of    61: 71% (71/100)\n",
      "Test Accuracy of    62: 68% (68/100)\n",
      "Test Accuracy of    63: 60% (60/100)\n",
      "Test Accuracy of    64: 41% (41/100)\n",
      "Test Accuracy of    65: 55% (55/100)\n",
      "Test Accuracy of    66: 71% (71/100)\n",
      "Test Accuracy of    67: 52% (52/100)\n",
      "Test Accuracy of    68: 92% (92/100)\n",
      "Test Accuracy of    69: 75% (75/100)\n",
      "Test Accuracy of    70: 69% (69/100)\n",
      "Test Accuracy of    71: 81% (81/100)\n",
      "Test Accuracy of    72: 37% (37/100)\n",
      "Test Accuracy of    73: 57% (57/100)\n",
      "Test Accuracy of    74: 51% (51/100)\n",
      "Test Accuracy of    75: 88% (88/100)\n",
      "Test Accuracy of    76: 90% (90/100)\n",
      "Test Accuracy of    77: 59% (59/100)\n",
      "Test Accuracy of    78: 48% (48/100)\n",
      "Test Accuracy of    79: 70% (70/100)\n",
      "Test Accuracy of    80: 50% (50/100)\n",
      "Test Accuracy of    81: 74% (74/100)\n",
      "Test Accuracy of    82: 83% (83/100)\n",
      "Test Accuracy of    83: 67% (67/100)\n",
      "Test Accuracy of    84: 56% (56/100)\n",
      "Test Accuracy of    85: 72% (72/100)\n",
      "Test Accuracy of    86: 65% (65/100)\n",
      "Test Accuracy of    87: 75% (75/100)\n",
      "Test Accuracy of    88: 69% (69/100)\n",
      "Test Accuracy of    89: 77% (77/100)\n",
      "Test Accuracy of    90: 73% (73/100)\n",
      "Test Accuracy of    91: 78% (78/100)\n",
      "Test Accuracy of    92: 59% (59/100)\n",
      "Test Accuracy of    93: 41% (41/100)\n",
      "Test Accuracy of    94: 91% (91/100)\n",
      "Test Accuracy of    95: 65% (65/100)\n",
      "Test Accuracy of    96: 55% (55/100)\n",
      "Test Accuracy of    97: 64% (64/100)\n",
      "Test Accuracy of    98: 45% (45/100)\n",
      "Test Accuracy of    99: 68% (68/100)\n",
      "Test Accuracy (Overall): 66% (6602/10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 确保模型在评估模式\n",
    "\n",
    "model0.eval()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "\n",
    "# 获取每个模型的概率预测\n",
    "prob_preds0 = np.array(model0.predict_poss(test_loader, use_cuda))\n",
    "prob_preds1 = np.array(model1.predict_poss(test_loader, use_cuda))\n",
    "prob_preds2 = np.array(model2.predict_poss(test_loader, use_cuda))\n",
    "prob_preds3 = np.array(model3.predict_poss(test_loader, use_cuda))\n",
    "prob_preds4 = np.array(model4.predict_poss(test_loader, use_cuda))\n",
    "\n",
    "# 将所有模型的概率相加\n",
    "summed_probs = prob_preds0 + prob_preds1 + prob_preds2 + prob_preds3 + prob_preds4\n",
    "\n",
    "# 选择概率总和最大的类别作为最终预测\n",
    "test_preds = np.argmax(summed_probs, axis=1)\n",
    "\n",
    "# 确保数据集具有属性 'targets' 或根据您的数据集结构进行调整\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "for i, label in enumerate(test_loader.dataset.targets):\n",
    "    class_correct[label] += int(test_preds[i] == label)\n",
    "    class_total[label] += 1\n",
    "\n",
    "# 打印每个类别的精度\n",
    "for i in range(100):\n",
    "    if class_total[i] > 0:\n",
    "        wandb.log({\"Res2Net_acc_ensemble_possibility\": class_correct[i] / class_total[i]})\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], class_correct[i], class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % i)\n",
    "\n",
    "# 计算并打印总体精度\n",
    "overall_accuracy = 100. * sum(class_correct) / sum(class_total)\n",
    "print('Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * sum(class_correct) / sum(class_total), sum(class_correct), sum(class_total)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
